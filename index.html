<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-34B5JN0EVN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-34B5JN0EVN');
    </script>
    <!-- <link rel="favicon" type="image/x-icon" href="assets/images/highlighter.png"> -->
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/highlighter.png"> -->
    <link rel="icon" type="image/png" href="assets/images/highlighter.png">
    <link rel="icon" type="image/x-icon" href="assets/images/highlighter.png">
    <link rel="shortcut icon" href="assets/images/highlighter.png">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HoT: Highlighted Chain of Thought</title>
    <meta name="description" content="A novel prompting approach for having LLMs highlight their own answers">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="Highlighted Chain of Thought" property="og:title">
    <meta content="A novel prompting approach for having LLMs highlight their own answers" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description" content="A novel prompting approach for having LLMs highlight their own answers">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script> <!-- Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        .container.blog.main .teaser-slideshow .foreground {
            max-width: 100%;
            max-height: 70vh;
            width: 80% !important;
            height: auto !important;
            display: block;
            margin: 0 auto;
            object-fit: contain;
        }
    </style>
</head>

<body>
    <!-- Title Page -->
    <div class="container blog main" id="first-content" style="background-color: #E0E4E6;">
        <div class="titleContainer">
            <h1 class="title"><img class="logoContainer" src="assets/images/highlighter_hot2.png" alt="HoT Logo">HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs
            </h1>
        </div>
        <p class="author">
            <a href="https://tin-xai.github.io/">Tin Nguyen</a><sup>1,*</sup>, 
            <a href="https://loganbolton.github.io/">Logan Bolton</a><sup>1,*</sup>, 
            <a href="https://taesiri.com/">Mohammad Reza Taesiri</a><sup>2</sup>, 
            <a href="https://scholar.google.com/citations?user=FpFTduYAAAAJ&hl=en">Trung Bui</a><sup>3</sup>,
            <a href="https://anhnguyen.me/research/">Anh Totti Nguyen</a><sup>1</sup>
        </p>
        <p class="affiliations">
            <sup>*</sup>Equal contribution
        </p>
        <p class="affiliations">
            <sup>1</sup>Auburn University, <sup>2</sup>University of Alberta, <sup>3</sup>Adobe Research
        </p>
        <div class="university-logos">
            <img src="assets/figures/auburn_logo.png" alt="Auburn University">
            <img src="assets/figures/alberta_logo.png" alt="University of Alberta">
            <img src="assets/figures/adobe_logo.png" alt="Adobe Research">
        </div>


        <div class="main-buttons">
            <a href="https://arxiv.org/pdf/2503.02003v4" class="button">
                Paper <i class="fa-solid fa-book"></i>
            </a>
            <a href="https://github.com/anguyen8/hot" class="button">
                Source Code <i class="fa-solid fa-code"></i>
            </a>
        </div>

        <div class="blog-cover">
            <div class="teaser-slideshow">
                <div class="slideshow-container">
                    <div class="slide active">
                        <img class="foreground" src="assets/figures/teaser_cot_hot.png" alt="HoT Teaser - MATH500 Example">
                        <p class="caption">CoT and HoT (ours) responses for a MATH500 question in ReasoningTrap benchmarks, both generated by <img class="llm" src="assets/images/models/geminiflash.png" alt="Gemini">. Left: With CoT prompting, the LLM misses the provided facts $a=10, b=c=d=0$, resulting in an incorrect answer. Right: With HoT prompting, the model highlights the key constraint $b=c=d=0$ and plugs it into the expression $ab^2c^3d^4$ to produce the correct answer of 0. </p>
                    </div>
                    <div class="slide">
                        <img class="foreground" src="assets/figures/puzzletrivial_example.png" alt="HoT Teaser - Puzzle Example">
                        <p class="caption">Left: After finetuned via SFT on CoT examples, Qwen-2.5-1.5B answers incorrectly an adversarial question from PuzzleTrivial as it does not factor in the key fact of permanently infertile lions.
                            Right:
                            In contrast, HoT-finetuned counterpart LLM can highlight facts and answer correctly using the fact the lions would never reproduce.</p>
                    </div>
                </div>
                <div class="slideshow-navigation">
                    <button class="nav-btn prev-btn" onclick="changeSlide(-1)">
                        <i class="fa-solid fa-chevron-left"></i>
                    </button>
                    <div class="slide-indicators">
                        <span class="indicator active" onclick="currentSlide(1)"></span>
                        <span class="indicator" onclick="currentSlide(2)"></span>
                    </div>
                    <button class="nav-btn next-btn" onclick="changeSlide(1)">
                        <i class="fa-solid fa-chevron-right"></i>
                    </button>
                </div>
            </div>
        </div>
    </div>

    <div class="container blog main">
        <h1 id="abstract">ðŸ“œ Abstract</h1>
        <p class="abstract">
            An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain of Thought Prompting (HoT), <b>a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query</b>. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in fewshot settings, <b>HoT outperforms vanilla chain of thought prompting</b> (CoT) (<a class="small-text" href="https://arxiv.org/abs/2201.11903">Wei et al., 2022</a>) on a wide range of 22 tasks from arithmetic, reading comprehension to logical reasoning. We also test how much highlights help users detect when LLMs are correct. As expected, they <b>help time-limited human participants to more accurately and efficiently recognize when LLMs are correct</b>. However, interestingly, when LLMs are wrong, HoTs tend to fool users into believing that an answer is correct.
        </p>
    </div>

    <div class="container blog main">
        <h1>
            Key Takeaways
        </h1>

        <ul>
            <li><b>HoT consistently improves accuracy</b> on arithmetic, question-answering, and logical reasoning problems across 5 different models and 22 datasets.</li>

            <li>The highlighted answers from HoT <b>improve humans' ability to verify correct LLM answers</b> by <span class="good">5.66</span> points (78.82% to 84.48%).</li>
            <li>HoT <b>reduces the amount of time needed to verify LLM answers</b> by <span class="good">+15.12 seconds</span> per question as compared to CoT (62.38 seconds to 47.26 seconds).</li>

            <li>HoT reduces hallucinations compared to CoT as measured by SelfCheckGPT.</li>
            <li>HoT outperforms other advanced prompting techniques (Least-to-Most (LtM),
                Tree-of-Thought (ToT), Self-Refine, and Chain-of-Verification
                (CoVE).
                Furthermore, when combined with Self-Consistency, ComplexCoT methods, HoT achieves superior performance than each of these methods alone and HoT alone</li>
        </ul>

    </div>

    <div class="container blog main">
        <h1>Recipe to make these highlights?</h1>
        <p class="text">
            To prompt LLMs to generate HoTs, we create the 8-shot demonstration examples (which are CoT demonstrations but with XML tags) would show LLMs how to insert tags and answer questions. Second, the HoT instruction would be a short, explicit request that asks LLMs to insert tags into questions and answer it. 
        </p>
        <div class="columns-1">
            <img src="assets/figures/flow.png" style="width: 100%">
            <p class="caption">LLMs generate HoT responses by wrapping XML tags around the information that the model determines is the most important. Regex and CSS are then used to add color highlighting for the user to easily understand the response.</p>
        </div>
    </div>  

    <div class="container blog main">
        <h1 >
            Benchmark Improvements
        </h1>

        <p class="text">
            We evaluate HoT on <b>22 tasks</b> across arithmetic, question-answering, and logical reasoning datasets using Gemini-1.5-Pro (<img class="llm" src="assets/images/models/gemini.png">), Gemini-1.5-Flash (<img class="flash-llm" src="assets/images/models/geminiflash.png">), Llama-3.1-70B (<img class="llm" src="assets/images/models/llama70b.png">), Llama-3.1-405B (<img class="llm" src="assets/images/models/llama405b.png">) and GPT-4o (<img class="llm" src="assets/images/models/openai.png">).
        </p>    

        <img src="assets/images/reasoningtrap_results.png">
        <p class="caption">
            HoT outperforms CoT across 3 ReasoningTrap tasks, with significant gains in MATH500 (Conditioned Math) (<span class="good">+18.00</span> <img class="flash-llm" src="assets/images/models/geminiflash.png">, an example shown in <img class="llm" src="assets/images/models/gemini.png">) and PuzzleTrivial (<span class="good">+12.50</span> <img class="flash-llm" src="assets/images/models/geminiflash.png">, an example shown in <img class="llm" src="assets/images/models/gemini.png">).  
        </p>

        <img src="assets/images/math.png">
        <p class="caption">
            HoT consistently improves accuracy over CoT across arithmetic tasks. Notably, HoT achieves the largest performance gains in AQUA (<span class="good">+14.64</span> for Gemini-1.5-Flash <img class="flash-llm" src="assets/images/models/geminiflash.png">) and r-GSM (<span class="good">+12.73</span> for Gemini-1.5-Pro <img class="llm" src="assets/images/models/gemini.png">).  
        </p>
    </div>
    <div class="container blog main">
        <img src="qa.png">
        <p class="caption">
            HoT demonstrates consistent accuracy improvements over CoT across QA tasks (StrategyQA, SpartQA, Date) and Reading Comprehension tasks (DROP (Break) and DROP (Census)). The largest gains are observed in StrategyQA (<span class="good">+15.07</span> for Llama-3.1-70B <img class="llm" src="assets/images/models/llama70b.png">) and SpartQA (<span class="good">+11.88</span> for Gemini-1.5-Flash <img class="flash-llm" src="assets/images/models/geminiflash.png">).
        </p>
    </div>
    <div class="container blog main">
        <img src="assets/images/logic.png">
        <p class="caption">
            HoT outperforms CoT across the logical reasoning BBH subsets, with notable gains in Causal Judgment (<span class="good">+15.5</span> for GPT-4o <img class="llm" src="assets/images/models/openai.png">) and Five Object tasks (<span class="good">+6.00</span> for Gemini-1.5-Pro <img class="llm" src="assets/images/models/gemini.png">).</p>
    </div>
    
    <div class="container blog main">
        <h1>
            HoT helps users to read Long Context Questions easier
        </h1>
        <p class="text">
            LLMs are great at answering a wide variety of questions, but it can be annoying to read through the huge blocks of text that they tend to generate. Humans frequently add color highlighting to our writing to make it easier to read, so why not allow LLMs to do the same thing?
        </p>
    </div>
    <div class="container blog main">
        <div class="columns-2">
            <img src="cot_response.png" style="width: 100%">
            <img src="assets/images/hot_response.png" style="width: 100%">
        </div>
        <div class = "center-div">
            <p class="caption">Which of these two responses are easier to read? <br> (Left) CoT: you have to parse through all the irrelevant context to find the part of the conversation that you care about. (Right) HoT: you can almost instantly scan over the LLM response to see exactly where it drew its answer from. </p>
        </div>
    </div>
    
    <div class="container blog main">
        <h1>
            How does this affect the user experience?
        </h1>

        <p class="text">
            We find that HoT helps time-limited human participants to more accurately and efficiently recognize when LLMs are correct. However, when LLMs are wrong, HoT responses tend to fool users into believing that an answer is actually correct.

            <div class="table-wrapper">
                <table>
                <thead class="center">
                    <tr>
                    <th>Prompt</th>
                    <th>Avg Time <br>(seconds)</th>
                    <th>Verification Accuracy for<br>Correct LLM Responses âœ“</th>
                    <th>Verification Accuracy for<br>Incorrect LLM Responses âœ—</th>
                    </tr>
                </thead>
                <tbody class="center">
                    <tr>
                    <td>HoT</td>
                    <td><b>47.26</b></td>
                    <td><b>84.48% &plusmn; 20.28%</b></td>
                    <td>54.83% &plusmn; 30.13%</td>
                    </tr>
                    <tr>
                    <td>CoT</td>
                    <td>62.38</td>
                    <td>78.82% &plusmn; 28.26%</td>
                    <td><b>72.21% &plusmn; 21.99%</b></td>
                    </tr>
                </tbody>
                </table>
            </div>
            
        </p>
    </div>


    <div class="container blog main">
        <h1>
            HoT reduces hallucinations compared to CoT
        </h1>

        <p class="text">
            HoT prompting makes LLMs, here <img class="flash-llm" src="assets/images/models/geminiflash.png">, hallucinates consistently less over a diverse set of tasks.
            Table shows the SelfCheckGPT hallucination scores. 
        Lower is better.
        </p>
        <div class="columns-1">
            <img src="assets/figures/hallucination_rate.png" style="width: 70%">
            <p class="caption">An example of a question and answer from StrategyQA, where CoT hallucinates the fact "Vietnam War lasted from 1955 to 1975" whereas HoT detects the fact within the question that "War in Vietnam (1945-46)".</p>
            <img class="foreground" src="assets/figures/strategyqa_example.png">
        </div>
        
            
            <!-- <p class="caption">Left: After finetuned via SFT on CoT examples, Qwen-2.5-1.5B answers incorrectly an adversarial question from PuzzleTrivial as it does not factor in the key fact of permanently infertile lions.
                Right:
                In contrast, HoT-finetuned counterpart LLM can highlight facts and answer correctly using the fact the lions would never reproduce.</p> -->
    </div>

    <div class="container blog main">
        <h1>
            HoT outperforms other advanced prompting techniques
        </h1>

        <p class="text">
            Over 5 runs across 22 benchmarks, HoT consistently outperforms both CoT and Repeating Questions (RQ), and even CoT + Self-Consistency (SC), and ComplexCoT.
            HoT and HoT + SC also outperforms their counterparts (ComplexCoT and CoT + SC) showing that HoT can complement these methods.
        </p>
        <div class="columns-1">
            <img src="assets/figures/selfconsistency.png" style="width: 100%">
        </div>

        <p class="text">
            On average over 5 runs and 3 datasets, HoT alone is still the most performing method compared to all other advanced prompting methods of CoT, LtM, CoVE, Self-Refine, and ToT. Under other prompting techniques, we observe LLMs often miss critical facts (e.g., overlooking temporal indicators like "yesterday" in Date), causing incorrect answers.
            In contrast, LLMs tend to focus better on key facts under HoT prompting.
        </p>
        <div class="columns-1">
            <img src="assets/figures/advance_prompting.png" style="width: 100%">
        </div>
    </div>

    <!-- <div class="container blog main">
        <h1>
           Discussion
        </h1>

        <h2>
            Why does HoT work?
        </h2>
        <p class="text">
            Part of the increase of benchmark accuracy in HoT can likely be attributed to the LLM repeating the input question in its response. Several papers (<a href="https://arxiv.org/abs/2309.06275">Xu et al., 2024</a> and <a href="https://arxiv.org/abs/2309.10687">Mekala et al., 2024</a>) have shown that having the LLM repeat the input question before generating a response can improve performance. However, in our full paper we show that HoT has a higher performance than just repeating the question. 
        </p>

        <p class="text">
            We theorize that generating extra tokens (in this case XML tags) around key facts helps the LLM to focus its attention to the most important part of the context. It's also possible that by adding the XML tags, the LLM is more effectively able to recall specific information from earlier in its context window with less frequent hallucinations. However, more experimentation is needed to verify these claims.
        </p>
        <h2>
            What about Reasoning Models?
        </h2>
        <p class="text">
            So if HoT can help LLMs to more effectively reason, how do reasoning models respond to this prompting strategy? Given the relatively high cost to run Deepseek R1 and the low rate limits of Gemini-2.0-Flash-Thinking, we were only able to run evaluations on a subset of benchmarks. However, we see no benefit to using HoT with these reasoning models. 
        </p>

        <p class="text">
            Given that HoT relies on fewshot examples, the negative results for Deepseek align with the warnings from the creators of R1 that fewshot examples can actually hurt R1's performance (<a href="https://arxiv.org/abs/2501.12948">DeepSeek-AI, 2025</a>). Interestingly, the <code>thinking</code> tokens for R1 only contain XML tags about 10% of the time, regardless of whether or not its final answer does actually include XML tags. Gemini Flash 2.0 Thinking does not provide thinking tokens over the API, so we are not able to analyze its internal Chain-of-Thought.
        </p>

        <p class="text">
           If trained to use XML tags in its <code>thinking</code> tokens, HoT could potentially be a useful tool for reasoning models to ground facts over long contexts. However, the current reasoning models do not benefit from HoT.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Limitations
        </h2>
        <p class="text">
            HoT relies on few shot examples in order to demonstrate the desired output format to the LLM. On our Github, we have gathered plenty of fewshot examples that can be applied to most domains. However, if you have a niche task that you want to use HoT on, you must first construct fewshot examples (whether manually or through LLM generated examples). Future work could easily alleviate this issue with a finetuned model that produces HoT responses by default. 
        </p>
    </div> -->

    <!-- Footer Page -->
    <footer>
        <div class="container">
            <h1>
                <i class="fa-solid fa-book"></i> BibTeX
            </h1>
            <pre><code>@article{nguyen2025hot,
                title={HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs},
                author={Nguyen, Tin and Bolton, Logan and Taesiri, Mohammad Reza and Trung, Bui and Nguyen, Anh Totti},
                journal={arXiv preprint arXiv:2503.02003},
                year={2025}
              }</code></pre>
        </div>

        <div class="container">
            <h1>Acknowledgments</h1>
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>. Website content was written by Tin Nguyen and Logan Bolton.
            </p>
        </div>    
    </footer>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>    
    <script src="assets/scripts/main.js"></script>
    
    <!-- Teaser Slideshow JavaScript -->
    <script>
        let currentSlideIndex = 1;
        
        function showSlide(n) {
            const slides = document.querySelectorAll('.slide');
            const indicators = document.querySelectorAll('.indicator');
            
            if (n > slides.length) {
                currentSlideIndex = 1;
            }
            if (n < 1) {
                currentSlideIndex = slides.length;
            }
            
            // Hide all slides
            slides.forEach(slide => {
                slide.classList.remove('active');
            });
            
            // Remove active from all indicators
            indicators.forEach(indicator => {
                indicator.classList.remove('active');
            });
            
            // Show current slide and activate indicator
            if (slides[currentSlideIndex - 1]) {
                slides[currentSlideIndex - 1].classList.add('active');
            }
            if (indicators[currentSlideIndex - 1]) {
                indicators[currentSlideIndex - 1].classList.add('active');
            }
        }
        
        function changeSlide(n) {
            currentSlideIndex += n;
            showSlide(currentSlideIndex);
        }
        
        function currentSlide(n) {
            currentSlideIndex = n;
            showSlide(currentSlideIndex);
        }
        
        // Initialize slideshow
        document.addEventListener('DOMContentLoaded', function() {
            showSlide(currentSlideIndex);
        });
    </script>
    </html>
</body>
