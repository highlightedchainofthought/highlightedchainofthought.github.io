<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <!-- <link rel="favicon" type="image/x-icon" href="assets/images/highlighter.png"> -->
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/highlighter.png"> -->
    <link rel="icon" type="image/png" href="assets/images/highlighter.png">
    <link rel="icon" type="image/x-icon" href="assets/images/highlighter.png">
    <link rel="shortcut icon" href="assets/images/highlighter.png">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HoT: Highlighted Chain of Thought</title>
    <meta name="description" content="We've presented Clarity, a minimalist and elegant website template for AI research.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="https://shikun.io/projects/clarity" property="og:url">
    <meta content="Clarity" property="og:title">
    <meta content="Website Template for AI Research" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description" content="Clarity: A Minimalist Website Template for AI Research">
    <meta name="twitter:image:src" content="assets/figures/clarity.png">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script> <!-- Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
    <div class="container blog" id="first-content" style="background-color: #E0E4E6;">
        <!-- If you don't have a project cover: Change "blog-title" into "blog-title no-cover"  -->
        <div class="blog-title">
            <div class="blog-intro">
                <div>
                    <div class="titleContainer">
                        <h1 class="title">HoT: Highlighted Chain of Thought for Referencing Supportive Facts from Inputs</h1>
                        <img class="logoContainer" src="assets/images/highlighter_hot2.png">
                    </div>
                    <p class="author"><a href="https://ngthanhtin.github.io/">Tin Nguyen</a>, <a href="https://loganbolton.github.io/">Logan Bolton</a>, <a href="https://taesiri.com/">Mohammad Reza Taesiri</a>, <a href="https://anhnguyen.me/research/">Anh Totti Nguyen</a></p>
                    <p class="abstract">
                        An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), <b>a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query</b>. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, <b>HoT outperforms vanilla chain of thought prompting</b> (CoT) (Wei et al., 2022)on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. We also test how much highlights help users detect when LLMs are correct. As expected, they <b>help time-limited human participants to more accurately and efficiently recognize when LLMs are correct</b>. However, interestingly, when LLMs are wrong, HoTs tend to fool users into believing that an answer is correct.
                    </p>
                   
                </div>
               
                <div class="info flex">
                    <div class="main-buttons">
                        <a href="https://github.com/lorenmt/clarity-template" class="button icon" style="background-color: rgba(255, 255, 255, 0.25); margin-bottom: 0;">Paper <i class="fa-solid fa-book"></i></a>
                        <a href="https://github.com/ngthanhtin/textual_grounding" class="button icon" style="background-color: rgba(255, 255, 255, 0.25); margin-bottom: 0;">Source Code <i class="fa-solid fa-code"></i></a>
                    </div>
                </div>
            </div>

            <div class="blog-cover">
                <img class="foreground" src="assets/figures/teaser2.png">
                <p class="caption">A HoT response from Gemini 1.5 Pro <img class="llm" src="assets/images/models/gemini.png"> in response to a question from GSM8K.</p>
                <!-- <img class="background" src="assets/figures/teaser2.png"> -->
                <!-- <img class="foreground" src="assets/figures/clarity.png"> -->
                <!-- <img class="background" src="assets/figures/clarity.png"> -->
            </div>
        </div>
    </div>


    <div class="container blog main first" id="blog-main">
        <h1>
            Introduction
        </h1>
        <p class='text'>
           Modern LLMs can bold, italicize or underline their text. Why not highlight as well? We propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate highlights around their responses that links specific information from the user query to the LLM response. This method <b>improves user experiences</b> and <b>improves answer accuracy</b> when compared to CoT. 
        </p>

    </div>

    <div class="container blog main">
        <h1>
            Key Takeaways
        </h1>
        <!-- <p class="text">
            In Clarity, I have provided container widths with five options: <code>main</code>, <code>large</code>, <code>extra-large</code>, <code>extra-extra-large</code>, and <code>max</code>. These container layouts are designed to be responsive, automatically adjusting based on the screen size.  The default width is <code>main</code>, which is used for this blog post. Be cautious when using the <code>max</code> option, as it has zero padding. Unless you are certain of the design, it's always recommended to leave some space for visual aesthetic purposes.
        </p> -->

        <ul>
            <li><b>HoT consistently improves accuracy</b> across 5 models and 17 datasets. 
                In arithmetic, question-answering, and logical reasoning problems, HoT achieves <span class="good">+1.60</span>, <span class="good">+2.58</span>, and <span class="good">+2.53</span> percentage points, respectively, as compared to vanilla few-shot CoT.</li>
            <li>Highlighted answers from HoT <b>improve human verification accuracy</b> by <span class="good">5.66</span> points (78.82% to 84.48%) and <b>reduce verification time</b> by <span class="good">-15.12 seconds</span> per question (62.38 seconds to 47.26 seconds) when assessing correct LLM responses compared to CoT answers</li>
            <li>Larger models generate highlights that <b>closely mirror human annotations</b> without requiring fine tuning.</li>
        </ul>

        <!-- <p class="text">
        Here is an example I designed, illustrating the architectural details in <a href="https://shikun.io/projects/prismer">Prismer</a> using the <code>extra-large</code> container width.
        </p> -->
    </div>



    <!-- <div class="container blog main">
        <h1 >
            Containers and Visual Diagrams
        </h1>

        <p class="text">
            Most AI projects incorporate visual diagrams to effectively communicate complex concepts. These diagrams often highlight new neural architecture designs and ML training pipelines, which are central to the project's contributions and research highlights. Clear and well-designed diagrams can significantly enhance the reader's understanding and engagement with the research. In Clarity, texts and visual diagrams are wrapped within a div container to maintain consistent design layouts.
        </p>      
        
        <p class="text">
            Simple design modules, such as minor adjustments to a neural network block, can be directly embedded within the text in the same container, without additional captions. This approach helps readers comprehend the proposed concept in a seamless and integrated manner.
        </p>

        <p class="text">
            Here is an example I redesigned, illustrating the difference between standard and bottleneck residual blocks as proposed in <a href="https://arxiv.org/abs/1512.03385">Residual Networks</a>.
        </p>

        <img src="clarity/images/residual.png" style="width: 80%;">

        <p class="text">
            For more complex visual diagrams, such as detailed neural architecture designs, it is recommended to use a separate container with a distinct background colour. This ensures that diagrams stand out and are easily distinguishable from the main text. Additionally, a new caption style is provided to help explain each design detail clearly.
        </p>

        <p class="text">
        Here is an example I redesigned, illustrating the architectural details in <a href="https://arxiv.org/abs/2010.11929">Vision Transformers (ViTs)</a>.
        </p>
    </div> -->


    <div class="container blog main">
        <h1 >
            Benchmark Improvements
        </h1>

        <p class="text">
            We evaluate HoT on <b>17 tasks</b> across arithmetic, question-answering, and logical reasoning datasets using Gemini-1.5-Pro (<img class="llm" src="assets/images/models/gemini.png">), Gemini-1.5-Flash (<img class="flash-llm" src="assets/images/models/geminiflash.png">), Llama-3.1-70B (<img class="llm" src="assets/images/models/llama70b.png">), Llama-3.1-405B (<img class="llm" src="assets/images/models/llama405b.png">) and GPT-4o (<img class="llm" src="assets/images/models/openai.png">).
        </p>    

        <img src="assets/images/math.png">
        <p class="caption">
            HoT consistently improves accuracy over CoT across arithmetic tasks. Notably, HoT achieves the largest performance gains in AQUA (<span class="good">+14.64</span> for Gemini-1.5-Flash) and r-GSM ((<span class="good">+12.73</span> for Gemini-1.5-Pro).  
        </p>
    </div>
    <div class="container blog main">
        <img src="assets/images/qa.png">
        <p class="caption">
            HoT demonstrates consistent accuracy improvements over CoT across QA tasks (StrategyQA, SpartQA, Date) and Reading Comprehension tasks (DROP (Break) and DROP (Cencus)). The largest gains are observed in StrategyQA (<span class="good">15.07</span> for Llama-3.1-70B) and SpartQA (<span class="good">11.88</span> for Gemini-1.5-Flash).
        </p>
    </div>
    <div class="container blog main">
        <img src="assets/images/logic.png">
        <p class="caption">
            \hot outperforms CoT across logical reasoning BBH subsets, with notable gains in Judgment ( \increasenoparent{15.5} for \gptlogo) and Five Object tasks (\increasenoparent{6.00} for \geminilogo).         </p>
    </div>
    <div class="container blog main">
        <h2>
           Adding XML Tags to LLM Responses Boosts Performance
        </h2>
        <p class="text">
            We found that adding XML tags to LLM responses significantly boosts performance across a wide range of tasks. In particular, we observed that the highlights generated by HoT closely mirror human annotations
        </p>
    </div>
    
    <div class="container blog main">
        <div class="columns-2">
            <img src="assets/images/ex1.png" style="width: 100%">
            <img src="assets/images/ex2.png" style="width: 100%">
        </div>
    </div>
    <div class="container blog main">
        <h1>
            What's the Point?
        </h1>
        <!-- <p class="text">
            In Clarity, I have provided container widths with five options: <code>main</code>, <code>large</code>, <code>extra-large</code>, <code>extra-extra-large</code>, and <code>max</code>. These container layouts are designed to be responsive, automatically adjusting based on the screen size.  The default width is <code>main</code>, which is used for this blog post. Be cautious when using the <code>max</code> option, as it has zero padding. Unless you are certain of the design, it's always recommended to leave some space for visual aesthetic purposes.
        </p> -->
        <p class="text">
            LLMs are great at giving you intelligent responses to any question you have. However, it can be annoying to read through huge blocks of text. Humans frequently add color highlighting to our writing to make it easier to read, so why not allow LLMs to do the same thing?
        </p>
    </div>
    <div class="container blog main">
        <div class="columns-2">
            <img src="assets/images/cot_response.png" style="width: 100%">
            <img src="assets/images/hot_response.png" style="width: 100%">
        </div>
    </div>
    <div class="container blog main">
        <p class="text">
            Consider this example. Which of these two responses are easier to read? For the response on the left, you have to parse through all the irrelevant context to find the part of the conversation that you care about. For the HoT response, you can almost instantly scan over the LLM responses to see exactly where it drew its answer from. 
        </p>

        <!-- <p class="text">
        Here is an example I designed, illustrating the architectural details in <a href="https://shikun.io/projects/prismer">Prismer</a> using the <code>extra-large</code> container width.
        </p> -->
    </div>

    <!-- <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png">
        <p class="caption">
            Prismer has two main trainable components: the <b>Experts Resampler</b> that converts variable multi-task signals to a fixed number of outputs, and the <b>Adaptor</b> that enhances the model's expressivity for vision-language reasoning. To ensure that the model takes advantage of the rich domain-specific knowledge encoded in the pre-trained experts, the majority of network weights are frozen during training, as represented by the snowflake icon.
        </p>
    </div> -->
    <div class="container blog main">
        <h1>How do you make these highlights?</h1>
        <p class="text">
            Given an input question, LLMs first repeat the original question but with XML tags wrapped around the key facts needed to answer the question. Then, the model generates answers using the information in the question. The model is able to understand what types of information to tag using fewshot examples given in the HoT instruction prompt.
        </p>
    </div>  
    
    <div class="container blog large">
        <div class="columns-1">
            <img src="assets/images/figure_hot_transform.png" style="width: 100%">
        </div>
    </div>

    <div class="container blog main">
        <p class="text">
            Take this question for example. First, the model reads over the original question. Then it repeats the question with XML tags wrapped around the most important facts. Since the model wrapped "2 books" with the <code>fact1</code> tag in the reformatted question, it knows to wrap any references to "2 books" in the answer with the same <code>fact1</code> tag.
        </p>

        <p class="text">
            Then using regex and CSS, we can easily strip out these XML tags and assign a highlight color to each unique tag. 
        </p>
    </div>
    <div class="container blog main">
    </div>
    <div class="container blog main">
        <h1>
            How does this affect the user experience?
        </h1>

        <p class="text">
            We conduct a study with <b>63 people</b> to evaluate the effectiveness of HoT in helping users verify the correctness of LLM answers. Users are randomly assigned to see exclusively HoT or CoT LLM response and then have to predict if the given answer is correct or incorrect. We found that HoT helps time-limited human participants to more accurately and efficiently recognize when LLMs are correct. However, when LLMs are wrong, HoTs tend to fool users into believing that an answer is correct.
            
            <!-- <div class="table-wrapper">
                <table>
                    <thead class="center">
                        <tr>
                            <th>Params</th>
                            <th >Dimension</th>
                            <th>$n$ heads</th>
                            <th>$n$ layers</th>
                            <th >Learning Rate</th>
                            <th>Batch Size</th>
                            <th>$n$ Tokens</th>
                        </tr>
                    </thead>
                    <tbody class="center">
                        <tr>
                            <td>6.7B</td>
                            <td>4096</td>
                            <td>32</td>
                            <td>32</td>
                            <td>$3e^{-4}$</td>
                            <td>4M</td>
                            <td>1T</td>
                        </tr>
                        <tr>
                            <td>13.0B</td>
                            <td>5120</td>
                            <td>40</td>
                            <td>40</td>
                            <td>$3e^{-4}$</td>
                            <td>4M</td>
                            <td>1T</td>
                        </tr>
                        <tr>
                            <td>32.5B</td>
                            <td>6656</td>
                            <td>52</td>
                            <td>60</td>
                            <td>$1.5e^{-4}$</td>
                            <td>4M</td>
                            <td>1T</td>
                        </tr>
                        <tr>
                            <td>65.2B</td>
                            <td>8192</td>
                            <td>64</td>
                            <td>80</td>
                            <td>$1.5e^{-4}$</td>
                            <td>4M</td>
                            <td>1T</td>
                        </tr>
                    </tbody>
                </table>
            </div> -->
            
            <div class="table-wrapper">
                <table>
                <thead class="center">
                    <tr>
                    <th>Prompt</th>
                    <th>Avg Time <br>(seconds)</th>
                    <th>Accuracy (%)<br>LLM is Correct ✓</th>
                    <th>Accuracy (%)<br>LLM is Incorrect ✗</th>
                    </tr>
                </thead>
                <tbody class="center">
                    <tr>
                    <td>HoT</td>
                    <td><b>47.26</b></td>
                    <td><b>84.48 &plusmn; 20.28</b></td>
                    <td>54.83 &plusmn; 30.13</td>
                    </tr>
                    <tr>
                    <td>CoT</td>
                    <td>62.38</td>
                    <td>78.82 &plusmn; 28.26</td>
                    <td><b>72.21 &plusmn; 21.99</b></td>
                    </tr>
                </tbody>
                </table>
            </div>
            
        </p>
    </div>


    <div class="container blog main">
        <h1>
           Discussion
        </h1>

        <h2>
            Why does HoT work?
        </h2>
        <p class="text">
            Part of the increase of accuracy in HoT can likely be attributed to repeating the question. Several papers (<a href="https://arxiv.org/abs/2309.06275">Xu et al., 2024</a> and <a href="https://arxiv.org/abs/2309.10687">Mekala et al., 2024</a>) have shown that having the LLM repeat the input question before generating a response can improve performance. However, in our full paper we show that HoT has a higher performance than just repeating the question. 
        </p>

        <p class="text">
            We theorize that by generating extra tokens (in this case XML tags) around key facts helps the LLM to focus its attention to the most important context. Its also possible that by adding the XML tags, the LLM is more effectively able to recall specific information from earlier in its context window with less frequent hallucinations. However, more research is needed to verify these claims.
        </p>
        <h2>
            What about Reasoning Models?
        </h2>
        <p class="text">
            So if HoT can help LLMs to more effectively reason, how do reasoning models respond to this prompting strategy? Given the relatively high cost to run Deepseek R1 and the low rate limits of Gemini-2.0-Flash-Thinking, we were only able to run evaluations on a subset of benchmarks. However, we see no benefit to using HoT with these reasoning models. 
            
            <!-- <div class="table-wrapper">
                <table>
                  <thead class="center">
                    <tr>
                      <th colspan="5"><h3>DeepSeek R1</h3></th>
                    </tr>
                    <tr>
                      <th>Prompt</th>
                      <th>r-GSM</th>
                      <th>DROP <br>(Break)</th>
                      <th>GSM <br>Symbolic</th>
                      <th>BBH: <br>Seven Objects</th>
                    </tr>
                  </thead>
                  <tbody class="center">
                    <tr>
                      <td>CoT (0-shot)</td>
                      <td>92</td>
                      <td>84</td>
                      <td><b>90</b></td>
                      <td>90.5</td>
                    </tr>
                    <tr>
                      <td>CoT (fewshot)</td>
                      <td>92.5</td>
                      <td>87</td>
                      <td>83</td>
                      <td><b>94.5</b></td>
                    </tr>
                    <tr>
                      <td>HoT (fewshot)</td>
                      <td><b>93</b></td>
                      <td><b>90</b></td>
                      <td>85.5</td>
                      <td><b>94.5</b></td>
                    </tr>
                  </tbody>
                </table>
              </div> -->
              
              <!-- <div class="table-wrapper">
                <table>
                  <thead class="center">
                    <tr>
                      <th colspan="5"><h3>Gemini-Flash-2.0-Thinking</h3></th>
                    </tr>
                    <tr>
                      <th>Prompt</th>
                      <th>r-GSM</th>
                      <th>DROP <br>(Break)</th>
                      <th>GSM <br>Symbolic</th>
                      <th>BBH: <br>Seven Objects</th>
                    </tr>
                  </thead>
                  <tbody class="center">
                    <tr>
                      <td>CoT (0-shot)</td>
                      <td>91</td>
                      <td>85</td>
                      <td><b>91.5</b></td>
                      <td><b>100</b></td>
                    </tr>
                    <tr>
                      <td>CoT (fewshot)</td>
                      <td><b>92</b></td>
                      <td><b>90.5</b></td>
                      <td>88</td>
                      <td>97.5</td>
                    </tr>
                    <tr>
                      <td>HoT (fewshot)</td>
                      <td>91.5</td>
                      <td>90</td>
                      <td>89.5</td>
                      <td>97</td>
                    </tr>
                  </tbody>
                </table>
              </div> -->
              
        </p>

        <p class="text">
            Given that HoT relies on fewshot examples, the negative results for Deepseek align with the warnings from the creators of R1 explicitly claim that fewshot examples can actually R1's performance. Interestingly, the <code>thinking</code> tokens for R1, do NOT contain XML tags, but its final answer does include XML tags. Gemini Flash 2.0 Thinking does not provide thinking tokens over the API, so we are not able to analyze its internal Chain of Thought.
        </p>

        <p class="text">
           If trained to use XML tags in its <code>thinking</code> tokens, HoT could potentially be a useful tool for reasoning models to ground facts over long contexts. However, the current reasoning models do not benefit from HoT.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Limitations
        </h2>
        <p class="text">
            The biggest limitation for HoT is its reliance on few shot examples. On our Github, we have gathered plenty of fewshot examles that can be applied to most domains. However if you have a niche task that you want to apply HoT on, you have to first construct the fewshot examples yourself. However, this could be easily fixed in future work with a finetuned model that produces HoT responses by default. 
        </p>
    </div>


    <div class="container blog main">
        <h1>
           Conclusion 
        </h1>
        <p class="text">
            We present Highlighted Chain of Thought, a novel prompting approach that enables LLMs to directly reference text from the input question in their responses. Our experiments show that on average, HoT improves arithmetic, question answering, and logical reasoning tasks by <span class="good">+1.6</span>, <span class="good">+2.58</span>, and <span class="good">+2.53</span> percentage points over CoT while also enabling users to verify correct LLM answers <span class="good">24% faster</span> than CoT LLM answers.
        </p>
    </div>
    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>. Web page content was written by Logan Bolton.
            </p>
        </div>    
    </footer>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>    
    <script src="assets/scripts/main.js"></script>    
    </html>
</body>